---
title: Muhan Li
role: Master of Science CS student @ Northwestern University
avatar_filename: face.jpg
bio: ""
interests:
  - Artificial Intelligence
  - Computational Linguistics
  - Information Retrieval
social:
  - icon: envelope
    icon_pack: fas
    link: /#contact
  - icon: twitter
    icon_pack: fab
    link: https://twitter.com/GeorgeCushen
  - icon: graduation-cap
    icon_pack: fas
    link: https://scholar.google.co.uk/citations?user=sIwtMXoAAAAJ
  - icon: github
    icon_pack: fab
    link: https://github.com/gcushen
  - icon: linkedin
    icon_pack: fab
    link: https://www.linkedin.com/
organizations:
  - name: Stanford University
    url: https://www.stanford.edu/
education:
  courses:
    - course: PhD in Artificial Intelligence
      institution: Stanford University
      year: 2012
    - course: MEng in Artificial Intelligence
      institution: Massachusetts Institute of Technology
      year: 2009
    - course: BSc in Artificial Intelligence
      institution: Massachusetts Institute of Technology
      year: 2008
email: ""
superuser: true
highlight_name: false
---
I’m an M.S. student in Computer Science at Northwestern University focusing on developing interpretable and generalizable intelligent systems, especially in the domain of NLP and RL, solving tasks such as reasoning and planning which usually involve processing abilities on a higher level of abstraction. (Eg: Searching was a research topic in AI, Image and Voice recognition are on the edge of being evicted from AI, AI is always a dynamic topic that shifts from time to time, and tasks are becoming more general in recent years).

My current goal of research could be expressed as 2 divisions: 

Develop algorithms that produce intrinsically interpretable models. 

<details>
<summary>Details on this topic</summary>

There exist many ways to improve the interpretability of intelligent algorithms, which are outlined clearly in this paper. Intrinsically interpretable models are one of them that could possibly help humans learn structured knowledge in the process of interpretation. Methods used in this area include imposing sparsity constraints such as limiting related representations \[1] and using simpler surrogate models \[2] \[3] \[4] \[5], or using causality relations \[6], and hierarchical learning \[7] \[8].

</details>

2. Design generalizable representations for intelligent systems to express, and access knowledge.

<details>
<summary>Details on this topic</summary>

While current deep neural models perform incredibly well on raw features, they lack generalizability when dealing with inputs from different modalities. This survey provides an overview of joint and coordinated representations used to cope with the problem, but there exists a vast amount of structured knowledge sources, such as knowledge bases, relational/non-relational databases, apart from popular datasets. Incorporating these more complex forms of knowledge requires specially engineered methods \[9] \[10].
</details>

{{< icon name="download" pack="fas" >}} Download my {{< staticref "uploads/demo_resume.pdf" "newtab" >}}resumé{{< /staticref >}}.