---
title: Muhan Li
role: Master of Science CS student
avatar_filename: avatar.jpg
bio: ""
interests:
  - Artificial Intelligence
  - Natural Language Processing
  - Reinforcement Learning
social:
  - icon: envelope
    icon_pack: fas
    link: mailto:muhanli2022@u.northwestern.edu
  - icon: github
    icon_pack: fab
    link: https://github.com/iffiX
  - icon: linkedin
    icon_pack: fab
    link: https://www.linkedin.com/in/%E6%B2%90%E6%99%97-%E6%9D%8E-5129111b7/
organizations:
  - name: Northwestern University
    url: https://www.northwestern.edu/
education:
  courses:
    - course: MSc in Computer Science
      institution: Northwestern University
      year: 2020
    - course: BSc in Computer Science
      institution: Sun Yat-sen University
      year: 2016
email: ""
superuser: true
highlight_name: false
---
I’m an M.S. student in Computer Science at Northwestern University focusing on developing interpretable and generalizable intelligent systems, especially in the domain of NLP and RL, solving tasks such as reasoning and planning which usually involve processing abilities on a higher level of abstraction. 

<details>
<summary>What is "higher level of abstraction"?</summary>
An example: Searching was a research topic in AI, Image and Voice recognition are on the edge of being evicted from AI, AI is always a dynamic topic that shifts from time to time, and tasks are becoming more general in recent years, involving abstract targets such as understanding the relations between entities in a visual scene, performing commonsense reasoning on the textual sequence and planning with human-designed abstract targets in RL.
</details>

My current goal of research could be expressed as 2 divisions: 

1. Develop algorithms that produce intrinsically interpretable models. 

   <details>
   <summary>Details on this topic</summary>

   There exist many ways to improve the interpretability of intelligent algorithms, which are outlined clearly in this [paper](https://www.mdpi.com/2079-9292/8/8/832/pdf). Intrinsically interpretable models are one of them that could possibly help humans learn structured knowledge in the process of interpretation. Methods used in this area include imposing sparsity constraints such as limiting related representations [(1)](https://arxiv.org/pdf/1809.04506.pdf) and using simpler surrogate models [(2)](https://arxiv.org/pdf/1807.05887.pdf) [(3)](https://arxiv.org/pdf/1610.05984.pdf) [(4)](https://arxiv.org/pdf/1712.04170.pdf) [(5)](https://arxiv.org/pdf/1804.02477.pdf), or using causality relations [(6)](https://arxiv.org/pdf/1811.00090.pdf), and hierarchical learning [(7)](https://arxiv.org/pdf/1611.01796.pdf) [(8)](https://arxiv.org/pdf/1712.07294.pdf).

   </details>

2. Design generalizable representations for intelligent systems to express, and access knowledge.

   <details>
   <summary>Details on this topic</summary>

   While current deep neural models perform incredibly well on raw features, they lack generalizability when dealing with inputs from different modalities. This [survey](https://arxiv.org/pdf/1705.09406.pdf) provides an overview of joint and coordinated representations used to cope with the problem, but there exists a vast amount of structured knowledge sources, such as knowledge bases, relational/non-relational databases, apart from popular datasets. Incorporating these more complex forms of knowledge requires specially engineered methods [(9)](https://arxiv.org/pdf/1909.07606.pdf) [(10)](https://arxiv.org/pdf/1909.08402.pdf).

   </details>

{{< icon name="download" pack="fas" >}} Download my {{< staticref "uploads/cv_muhan_li.pdf" "newtab" >}}resumé{{< /staticref >}}.