---
abstract: "We introduce a method to extend the vocabulary encoding of BERT with
  context encoding containing rich information of an input token, in a given
  sequence of text. The context encoding is output by another BERT model, named
  CTX-BERT, dedicated to infer relations to entities of the specified token in
  its context. To simplify the model, we combine three objectives: entity
  detection, entity encoding, and relation recovery into one by requiring
  CTX-BERT to recover the relation triples as a textual sequence when given a
  context sequence with the target token masked. Experimental results
  demonstrate that CTX-BERT could enhance the performance of the second BERT on
  question answering tasks. "
slides: ""
url_pdf: ""
publication_types:
  - "3"
  - "1"
authors:
  - admin
  - David Demeter
summary: We introduce a method to extend the vocabulary encoding of BERT with
  context encoding of a input token, with information from knowledge bases, in a
  given sequence of text.
"We introduce a method to extend the vocabulary encoding of BERT with context encoding containing rich information of a input token, in a given sequence of text. The context encoding is output by another BERT model, named as CTX-BERT, dedicated to infer relations to entities of the specified token in its context. To simplify the model, we combine there objectives":
  entity detection, entity encoding and relation recovery into one by requiring
  CTX-BERT to recover the relation triples as as a textual sequence when given a
  context sequence with the target token masked. Experimental results
  demonstrate that CTX-BERT could enhance the performance of the second BERT on
  question answering tasks.
url_dataset: ""
url_project: ""
author_notes: []
publication: ""
publication_short: In *ICLR 2022*
url_source: ""
url_video: ""
title: BERT with context information encoding from knowledge graphs
doi: ""
featured: true
tags: []
projects: []
image:
  caption: ""
  focal_point: ""
  preview_only: false
  filename: featured.png
date: 2021-08-19T04:17:16.742Z
url_slides: ""
publishDate: 2021-08-19T04:17:16.742Z
url_poster: ""
url_code: ""
---
