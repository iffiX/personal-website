---
abstract: >-
We introduce a method to extend the vocabulary encoding of BERT with context encoding containing rich information of a input token, in a given sequence of text. The context encoding is output by another BERT model, named as CTX-BERT, dedicated to infer relations to entities of the specified token in its context. To simplify the model, we combine there objectives: entity detection, entity encoding and relation recovery into one by requiring CTX-BERT to recover the relation triples as as a textual sequence when given a context sequence with the target token masked. Experimental results demonstrate that CTX-BERT could enhance the performance of the second BERT on question answering tasks. 

slides: ""
url_pdf: ""
publication_types:
  - "3"
  - "1"
authors:
  - admin
  - David Demeter
author_notes: []
publication: ""
summary: We introduce a method to extend the vocabulary encoding of BERT with context encoding of a input token, with information from knowledge bases, in a given sequence of text.
url_dataset: ""
url_project: ""
publication_short: In *ICLR 2022*
url_source: ""
url_video: ""
title: BERT with context information encoding from knowledge graphs
doi: ""
featured: true
tags: []
projects: []
image:
  caption: ""
  focal_point: ""
  preview_only: false
  filename: featured.png
date: 2021-08-19T04:17:16.742Z
url_slides: ""
publishDate: 2021-08-19T04:17:16.742Z
url_poster: ""
url_code: ""
---
